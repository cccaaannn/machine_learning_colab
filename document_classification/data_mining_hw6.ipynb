{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_mining_hw6",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0xUYcWhNson2fuy6WC3fe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cccaaannn/machine_learning_colab/blob/master/document_classification/data_mining_hw6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4RjMwY7gXMS"
      },
      "source": [
        "Download and unzip dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDhgIaGWosqI"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip\r\n",
        "!unzip YouTube-Spam-Collection-v1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkZkpJkLgbdA"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3akGoHA4ts3Q"
      },
      "source": [
        "# sklearn\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\n",
        "\r\n",
        "\r\n",
        "# nltk\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import nltk\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "# other\r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-vzSb6wgdM4"
      },
      "source": [
        "Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsM_Djdjts6B"
      },
      "source": [
        "Psy = pd.read_csv(\"Youtube01-Psy.csv\")\r\n",
        "KatyPerry = pd.read_csv(\"Youtube02-KatyPerry.csv\")\r\n",
        "LMFAO = pd.read_csv(\"Youtube03-LMFAO.csv\")\r\n",
        "Eminem = pd.read_csv(\"Youtube04-Eminem.csv\")\r\n",
        "Shakira = pd.read_csv(\"Youtube05-Shakira.csv\")\r\n",
        "\r\n",
        "dfs = [Psy, KatyPerry, LMFAO, Eminem, Shakira]\r\n",
        "\r\n",
        "combined_df = pd.concat(dfs)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmrOZxgegfvB"
      },
      "source": [
        "Dataset info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AedJ4SEIts9C"
      },
      "source": [
        "combined_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Vq2Cmaw1Ot"
      },
      "source": [
        "combined_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lubohdHfghnV"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjf3iaAK87ps"
      },
      "source": [
        "def test_and_report(model, X_test, y_test):\r\n",
        "    pred = model.predict(X_test)\r\n",
        "    print(confusion_matrix(y_test, pred))\r\n",
        "    print(classification_report(y_test, pred))\r\n",
        "    print(\"accuracy: {}\".format(accuracy_score(y_test, pred)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd63AOvMNhmq"
      },
      "source": [
        "def stemming(word):\r\n",
        "    stemmer = PorterStemmer()\r\n",
        "    lemmatizer = WordNetLemmatizer()\r\n",
        "    word = str(word)\r\n",
        "    if word == word.title():\r\n",
        "        word = stemmer.stem(word).capitalize()\r\n",
        "        word = lemmatizer.lemmatize(word).capitalize()\r\n",
        "    elif word.isupper():\r\n",
        "        word = stemmer.stem(word).upper()\r\n",
        "        word = lemmatizer.lemmatize(word).upper()\r\n",
        "    else:\r\n",
        "        word = stemmer.stem(word)\r\n",
        "        word = lemmatizer.lemmatize(word)\r\n",
        "    return word"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsUhat52P3wB"
      },
      "source": [
        "def rm_stop_words(word, case_sensitive=False):\r\n",
        "    if(word.lower() in stopwords.words(\"english\")):\r\n",
        "        return \"\"\r\n",
        "    else:\r\n",
        "        if(case_sensitive):\r\n",
        "            return word\r\n",
        "        else:\r\n",
        "            return word.lower()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fas9Q6VZJQQ7"
      },
      "source": [
        "def process_comments(raw_comments, special=False, single=True, nums=True, lowercase =True, stem=True, rm_stops=True): \r\n",
        "    comments = []\r\n",
        "    for comment in raw_comments:\r\n",
        "\r\n",
        "        # remove special characters\r\n",
        "        if(special):\r\n",
        "            comment = re.sub(r'\\W', ' ', comment)\r\n",
        "\r\n",
        "        # remove single characters\r\n",
        "        if(single):\r\n",
        "            comment = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', comment)\r\n",
        "\r\n",
        "        # remove numbers\r\n",
        "        if(nums):\r\n",
        "            comment = ''.join([i for i in comment if not i.isdigit()])\r\n",
        "\r\n",
        "        # to lowercase\r\n",
        "        if(lowercase):\r\n",
        "            comment = comment.lower()\r\n",
        "\r\n",
        "        if(stem):\r\n",
        "            comment = comment.split()\r\n",
        "            comment = [stemming(t) for t in comment]\r\n",
        "            comment = ' '.join(comment)\r\n",
        "        \r\n",
        "        if(rm_stops):\r\n",
        "            comment = comment.split()\r\n",
        "            comment = [rm_stop_words(t, case_sensitive=not lowercase) for t in comment]\r\n",
        "            comment = ' '.join(comment)\r\n",
        "\r\n",
        "        comments.append(comment)\r\n",
        "\r\n",
        "    return comments"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLFa0DVUgneB"
      },
      "source": [
        "Chose dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtUMAFth87jy"
      },
      "source": [
        "comments = process_comments(combined_df[\"CONTENT\"], lowercase=False)\r\n",
        "y = combined_df[\"CLASS\"]\r\n",
        "\r\n",
        "# comments = process_comments(combined_df[\"CONTENT\"])\r\n",
        "# y = combined_df[\"CLASS\"]\r\n",
        "\r\n",
        "# comments = process_comments(Eminem[\"CONTENT\"])\r\n",
        "# y = Eminem[\"CLASS\"]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgNBj3rPgqaX"
      },
      "source": [
        "Vectorizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsovxLCbD4gV"
      },
      "source": [
        "count_vectorizer = CountVectorizer(max_features=850, min_df=1, stop_words=stopwords.words(\"english\"))\r\n",
        "\r\n",
        "count_model = count_vectorizer.fit(comments)\r\n",
        "x = count_model.transform(comments)\r\n",
        "print(x.shape)\r\n",
        "print(count_model.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqw0uBbnz9f6"
      },
      "source": [
        "# tfidf_vectorizer = TfidfVectorizer(max_features=250, min_df=5, stop_words=stopwords.words(\"english\"), lowercase=False)\r\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=850, min_df=1, stop_words=stopwords.words(\"english\"))\r\n",
        "\r\n",
        "tfidf_model = tfidf_vectorizer.fit(comments)\r\n",
        "x = tfidf_model.transform(comments)\r\n",
        "print(x.shape)\r\n",
        "print(tfidf_model.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_CG_DD6gszk"
      },
      "source": [
        "Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaBiwIZZ384x"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True)\r\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o8EG2QSgvJb"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNQ_SbONxegR"
      },
      "source": [
        "one_vs_all = OneVsRestClassifier(LogisticRegression())\r\n",
        "one_vs_all.fit(X_train, y_train)\r\n",
        "\r\n",
        "random_forest = RandomForestClassifier()\r\n",
        "random_forest.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrn10A4Rgw0R"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olPkqv8EUkOL"
      },
      "source": [
        "print(\"one vs rest\")\r\n",
        "test_and_report(one_vs_all, X_test, y_test)\r\n",
        "\r\n",
        "print(\"random forest\")\r\n",
        "test_and_report(random_forest, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anKXbFBJg0zT"
      },
      "source": [
        "Testing model agains other singers (for not combined training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr5gPTw6Zs0j"
      },
      "source": [
        "for df in dfs:\r\n",
        "    test = process_comments(df[\"CONTENT\"])\r\n",
        "    test = tfidf_model.transform(test)\r\n",
        "\r\n",
        "    print(\"one vs all\\n\")\r\n",
        "    test_and_report(one_vs_all, test, df[\"CLASS\"])\r\n",
        "    print(\"\\nrandom forest\\n\")\r\n",
        "    test_and_report(random_forest, test, df[\"CLASS\"])\r\n",
        "    print(\"\\n\"+\"-\"*50+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}